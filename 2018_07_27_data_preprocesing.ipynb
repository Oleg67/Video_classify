{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import skvideo\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateListofImages(folder, max_length=50, f_lists=np.array([]), period=1):\n",
    "    \"\"\"\n",
    "    serve to make the list of video images with period \n",
    "    and max_length\n",
    "    return list of images from folder of images\n",
    "    \"\"\"\n",
    "    \n",
    "    f_list = np.sort(os.listdir(folder))\n",
    "    f_list = np.vectorize(lambda x: os.path.join(os.getcwd(), folder, x))(f_list)\n",
    "    ind_mask = np.arange(len(f_list)) % period == 0\n",
    "    \n",
    "    \n",
    "    while (np.sum(ind_mask) < max_length) & (period > 1):\n",
    "        period -=1\n",
    "        ind_mask = np.arange(len(f_list)) % period == 0\n",
    "        if (period ==1) & (np.sum(ind_mask) < max_length):\n",
    "            f_list = np.sort(np.random.choice(f_list, size=max_length))\n",
    "            return np.append(f_lists, f_list)\n",
    "        \n",
    "    \n",
    "    f_list = f_list[ind_mask]\n",
    "    for i in range(len(f_list) - max_length):\n",
    "        _f_list = f_list[i*max_length :(i+1)*max_length]\n",
    "        if len(_f_list) == 0:\n",
    "            break\n",
    "        if len(_f_list) < max_length:\n",
    "            _f_list = np.sort(np.random.choice(_f_list, size=max_length))\n",
    "            break\n",
    "        f_lists = np.append(f_lists, _f_list)\n",
    "        \n",
    "    return f_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "\n",
    "def CreateTensorFromListofImages(image_list, max_length):\n",
    "    \"\"\"\n",
    "    get the tensor from list of images\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.full((len(image_list), 197, 197, 3), 0, dtype=np.int32)\n",
    "    for i,image in enumerate(image_list):\n",
    "        X[i] = imread(image)\n",
    "         \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_images_featues(list_videos, model, max_length=50):\n",
    "    \"\"\"\n",
    "    from list of images build the data's features use model to get the image's features\n",
    "    \"\"\"\n",
    "    im_features = np.array([])\n",
    "    for f in list_videos:\n",
    "        \n",
    "        images_list = CreateListofImages(f, max_length=max_length)\n",
    "        X = CreateTensorFromListofImages(images_list, max_length)\n",
    "        X = X.reshape(X.shape[0]/max_length, max_length, 197, 197, 3)\n",
    "        _im_features = np.full((X.shape[0], X.shape[1], 1, 1 ,2048), 0, dtype=np.int32)\n",
    "        for i, x in enumerate(X):\n",
    "            _im_features[i] = model.predict_on_batch(x.reshape(X.shape[1], X.shape[2], X.shape[3], X.shape[4]))\n",
    "        if len(im_features) == 0:\n",
    "            im_features = _im_features\n",
    "        else:\n",
    "            im_features = np.vstack((im_features, _im_features))\n",
    "    return im_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_train_test(folder, test_size=0.1):\n",
    "    \"\"\"\n",
    "    split the folder of video on train and test \n",
    "    \"\"\"\n",
    "    \n",
    "    f_list = np.sort(os.listdir(os.path.join(os.getcwd(), folder)))\n",
    "    f_list = np.vectorize(lambda x: os.path.join(os.getcwd(), folder, x))(f_list)\n",
    "    _n_train = int(len(f_list) * test_size)\n",
    "    _train_list = f_list[: len(f_list) -  _n_train]\n",
    "    _test_list = f_list[len(f_list) -  _n_train :]\n",
    "    \n",
    "            \n",
    "    return _train_list, _test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_data (folder_list, model, max_length=50, period=1, verbose=False):\n",
    "    \"\"\"\n",
    "    serve to make the train test data as data and labels \n",
    "    folder_list - 2 folders contain video 2 labels\n",
    "    model use to extract features from images\n",
    "    max_length maximum length of images for one sapmle\n",
    "    period - as take the image from video\n",
    "    \"\"\"\n",
    "    for i,fd in enumerate(folder_list):\n",
    "        if i ==0:\n",
    "            train_video, test_video = Create_train_test(fd)\n",
    "            if verbose:\n",
    "                print 'test_size {}   {}'.format(len(test_video), fd)\n",
    "            X_train = Create_images_featues(train_video, model)\n",
    "            X_test = Create_images_featues(test_video, model)\n",
    "            y_train = np.full(len(X_train), dtype=np.int8, fill_value=i)\n",
    "            y_test = np.full(len(X_test), dtype=np.int8, fill_value=i)\n",
    "        else:\n",
    "            train_video, test_video = Create_train_test(fd)\n",
    "            if verbose:\n",
    "                print 'test_size {}  {}'.format(len(test_video), fd)\n",
    "            _X_train = Create_images_featues(train_video, model)\n",
    "            _X_test = Create_images_featues(test_video, model)\n",
    "            X_train = np.concatenate((X_train, _X_train), axis=0)\n",
    "            X_test = np.concatenate((X_test, _X_test), axis=0)\n",
    "            y_train = np.concatenate((y_train, np.full(len(_X_train), dtype=np.int8, fill_value=i)), axis=0)\n",
    "            y_test = np.concatenate((y_test, np.full(len(_X_test), dtype=np.int8, fill_value=i)), axis=0)\n",
    "            \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders_video = ['video_S197/video_good', 'video_S197/video_bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oleg/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oleg/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#libraries\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imRes = tf.keras.applications.ResNet50(include_top=False, input_shape=(197,197,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imRes.predict_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size 5   video_S197/video_good\n",
      "test_size 5  video_S197/video_bad\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_data(folders_video, imRes, max_length=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((689, 50, 1, 1, 2048), (689,), (60, 50, 1, 1, 2048), (60,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275600"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.nbytes/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('video_x_Res.npy', 'w'),X_train)\n",
    "np.save(open('video_y_Res.npy','w'),y_train)\n",
    "np.save(open('video_x_Res_val.npy', 'w'),X_test)\n",
    "np.save(open('video_y_Res_val.npy','w'),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Reshape((50,2048), input_shape=[50,1,1,2048] ))\n",
    "model.add(tf.keras.layers.LSTM(50, dropout=0.4))\n",
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 50, 2048)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                419800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 419,851\n",
      "Trainable params: 419,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 689 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 2/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 3/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 4/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 5/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 6/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 7/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 8/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 9/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 10/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 11/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 12/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 13/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 14/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 15/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 16/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 17/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 18/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 19/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 20/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 21/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 22/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 23/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 24/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 25/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 26/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 27/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 28/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 29/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 30/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 31/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 32/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 33/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 34/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 35/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 36/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 37/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 38/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 39/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 40/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 41/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 42/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 43/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 44/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 45/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 46/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 47/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 48/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 49/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n",
      "Epoch 50/50\n",
      " - 5s - loss: 8.3530 - acc: 0.4761 - val_loss: 2.9228 - val_acc: 0.8167\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=8,  \n",
    "          workers=8, \n",
    "                    epochs=50, \n",
    "                    verbose=2,\n",
    "                    use_multiprocessing=True,\n",
    "                    validation_data=(X_test ,   y_test)\n",
    "        \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
